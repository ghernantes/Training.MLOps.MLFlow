{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b9d4d3",
   "metadata": {},
   "source": [
    "# 2. How to deploy from MLflow with python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da14fd4a",
   "metadata": {},
   "source": [
    "## 2.1 MLflow Models\n",
    "\n",
    "An MLflow Model is a standard format for packaging machine learning models that can be used in a variety of downstream tools - for example: \n",
    "- real-time serving through a REST API, or \n",
    "- batch inference on Apache Spark\n",
    "\n",
    "The format defines a convention that lets you save a model in different “flavors” that can be understood by different downstream tools.\n",
    "\n",
    "All of the flavors that a particular model supports are defined in its MLmodel file in YAML format. For example, `mlflow.sklearn` outputs models as follows:\n",
    "\n",
    "```\n",
    "# Directory written by mlflow.sklearn.save_model(tree, \"model\")\n",
    "model/\n",
    "├── MLmodel\n",
    "├── model.pkl\n",
    "├── conda.yaml\n",
    "└── requirements.txt\n",
    "```\n",
    "\n",
    "For environment recreation, we automatically log `conda.yaml` and `requirements.txt` files whenever a model is logged. These files can then be used to reinstall dependencies using either `conda` or `pip`. \n",
    "\n",
    "And its `MLmodel` file describes two 'flavors':\n",
    "\n",
    "```yaml\n",
    "time_created: 2018-05-25T17:28:53.35\n",
    "\n",
    "flavors:\n",
    "  sklearn:\n",
    "    sklearn_version: 0.19.1\n",
    "    pickled_model: model.pkl\n",
    "  python_function:\n",
    "    loader_module: mlflow.sklearn\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30cbf0d0",
   "metadata": {},
   "source": [
    "This model can then be used with any tool that supports either the `sklearn` or `python_function` model flavor. For example, the `mlflow models serve` command can serve a model with the `python_function` flavor:\n",
    "\n",
    "```bash\n",
    "(mlflow)$ mlflow models serve -m model\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f577144e",
   "metadata": {},
   "source": [
    "## 2.2 The MLflow Model Registry\n",
    "\n",
    "The MLflow Model Registry component is a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model. It provides model lineage (which MLflow experiment and run produced the model), model versioning, stage transitions (for example from staging to production), and annotations.\n",
    "\n",
    "- **Model**: A MLflow Model is created from an experiment or run that is logged with one of the model flavor’s `mlflow.\\<model_flavor\\>.log_model()` methods. Once logged, this model can then be registered with the Model Registry.\n",
    "\n",
    "- **Registered Model**: A MLflow Model can be registered with the Model Registry. A registered model has a unique name, contains versions, associated transitional stages, model lineage, and other metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c1a65",
   "metadata": {},
   "source": [
    "## 2.3. Register a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c906e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "server_uri = \"http://localhost:5003\"     # port 5003: a local tracking server \n",
    "#server_uri = \"http://localhost:5007\"     # port 5007: a local docker container running a tracking server\n",
    "\n",
    "mlflow.set_tracking_uri(server_uri)       # or set the MLFLOW_TRACKING_URI in the env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f90a8504",
   "metadata": {},
   "source": [
    "First we need to create a registered model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94949859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'penguins_clf' already exists in registry.\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.exceptions import RestException\n",
    "\n",
    "model_name = \"penguins_clf\"\n",
    "\n",
    "client = MlflowClient()\n",
    "try:\n",
    "    registered_model = client.create_registered_model(model_name)\n",
    "    print(registered_model)\n",
    "except RestException:\n",
    "    print(f\"Model '{model_name}' already exists in registry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8511126",
   "metadata": {},
   "source": [
    "<img src='../img/mlflow_ui_pinguins_created_models_list.png' alt='' width='1000'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d2b26",
   "metadata": {},
   "source": [
    "Now we can register experiment runs to that model. Pick a run ID from your tracking log and add it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bf1e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'penguins_clf' already exists. Creating a new version of this model...\n",
      "2023/03/26 21:12:25 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: penguins_clf, version 1\n",
      "Created version '1' of model 'penguins_clf'.\n"
     ]
    }
   ],
   "source": [
    "# Use YOUR last run id:\n",
    "run_id = \"8e0f51052f4648f99bece2c8ae6e55cc\"\n",
    "\n",
    "result = mlflow.register_model(\n",
    "    f\"runs:/{run_id}/model\",\n",
    "    f\"{model_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bad46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ModelVersion: creation_timestamp=1679857945836, current_stage='None', description='', last_updated_timestamp=1679857945836, name='penguins_clf', run_id='8e0f51052f4648f99bece2c8ae6e55cc', run_link='', source='./mlruns/1/8e0f51052f4648f99bece2c8ae6e55cc/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='1'>\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f05b204",
   "metadata": {},
   "source": [
    "List of registered models:\n",
    "\n",
    "<img src='../img/mlflow_ui_pinguins_created_models_list_with_version.png' alt='' width='1000'>\n",
    "\n",
    "List of versions of the model `penguins_clf`:\n",
    "\n",
    "<img src='../img/mlflow_ui_pinguins_created_model_list_of_versions.png' alt='' width='1000'>\n",
    "\n",
    "Details of model `penguins_clf` Version 1:\n",
    "\n",
    "<img src='../img/mlflow_ui_pinguins_created_model_version_1_details.png' alt='' width='1000'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d3a28",
   "metadata": {},
   "source": [
    "## 2.4. Serve a Model from the registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea195735",
   "metadata": {},
   "source": [
    "**Open a new termina 4 and activate the `mlflow` conda env:**\n",
    "\n",
    "```bash\n",
    "$ conda activate mlflow\n",
    "(mlflow)$ \n",
    "```\n",
    "\n",
    "**Cd to your `lab2/mlflow` folder**\n",
    "\n",
    "```bash\n",
    "(mlflow)$ cd mlflow\n",
    "```\n",
    "\n",
    "**Serve a given model version:**\n",
    "\n",
    "```bash\n",
    "# Set environment variable for the tracking URL where the Model Registry resides\n",
    "# Serve the production model from the model registry\n",
    "\n",
    "(mlflow)$ export MLFLOW_TRACKING_URI=http://localhost:5003 \n",
    "(mlflow)$ mlflow models serve --no-conda -m \"models:/penguins_clf/1\" -p 4242\n",
    "\n",
    "2023/03/26 21:16:29 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
    "2023/03/26 21:16:29 INFO mlflow.pyfunc.backend: === Running command 'exec gunicorn --timeout=60 -b 127.0.0.1:4242 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n",
    "[2023-03-26 21:16:29 +0200] [2191368] [INFO] Starting gunicorn 20.1.0\n",
    "[2023-03-26 21:16:29 +0200] [2191368] [INFO] Listening at: http://127.0.0.1:4242 (2191368)\n",
    "[2023-03-26 21:16:29 +0200] [2191368] [INFO] Using worker: sync\n",
    "[2023-03-26 21:16:29 +0200] [2191415] [INFO] Booting worker with pid: 2191415\n",
    "```\n",
    "(This serves version 1 of the model)\n",
    "\n",
    "\n",
    "**Or alternatively, serve by model stage:**\n",
    "\n",
    "```bash\n",
    "(mlflow)$ export MLFLOW_TRACKING_URI=http://localhost:5003 \n",
    "(mlflow)$ mlflow models serve --no-conda -m \"models:/penguins_clf/Production\" -p 4242\n",
    "```\n",
    "\n",
    "(This serves the model in Production state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8f0d081",
   "metadata": {},
   "source": [
    "## 2.5 Query the REST API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "321c1f6a",
   "metadata": {},
   "source": [
    "The REST API defines 4 endpoints:\n",
    "\n",
    "- `/version` used for getting the mlflow version\n",
    "- `/ping` used for health check\n",
    "- `/health` (same as /ping)\n",
    "- `/invocations` used for scoring\n",
    "\n",
    "\n",
    "The REST API server accepts `csv` or `json` input. The input format must be specified in `Content-Type` header. The value of that header must be either `application/json` or `application/csv`.\n",
    "\n",
    "- **The `csv` input** must be a valid `pandas.DataFrame` csv representation. For example, `data = pandas_df.to_csv()`.\n",
    "\n",
    "- **The `json` input** must be a dictionary with exactly one of the following fields that further specify the type and encoding of the input data\n",
    "    - `dataframe_split` field with pandas DataFrames in the split orientation. For example: \n",
    "    \n",
    "      `data = {\"dataframe_split\": pandas_df.to_dict(orient='split')`\n",
    "\n",
    "    - `dataframe_records` field with pandas DataFrame in the records orientation. For example: \n",
    "    \n",
    "       `data = {\"dataframe_records\": pandas_df.to_dict(orient='records')`\n",
    "       \n",
    "       *We do not recommend using this format because it is not guaranteed to preserve column ordering.*\n",
    "       \n",
    "    - `instances` field with tensor input formatted as described in [TF Serving’s API docs](https://www.tensorflow.org/tfx/serving/api_rest#request_format_2) where the provided inputs will be cast to Numpy arrays.\n",
    "    - `inputs` field with tensor input formatted as described in [TF Serving’s API docs](https://www.tensorflow.org/tfx/serving/api_rest#request_format_2) where the provided inputs will be cast to Numpy arrays.\n",
    "\n",
    "> NOTE:\n",
    "> Since JSON loses type information, MLflow will cast the JSON input to the input type specified in the model’s schema if available. If your model is sensitive to input types, it is recommended that a schema is provided for the model to ensure that type mismatch errors do not occur at inference time. In particular, DL models are typically strict about input types and will need model schema in order for the model to score correctly. For complex data types, see Encoding complex data below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "542b6c69",
   "metadata": {},
   "source": [
    "**Query the API with cURL:**\n",
    "\n",
    "```bash\n",
    "$ curl http://127.0.0.1:4242/version\n",
    "2.1.1\n",
    "\n",
    "$ curl http://127.0.0.1:4242/health\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4ee1894",
   "metadata": {},
   "source": [
    "**Query the model with cURL:**\n",
    "\n",
    "```bash\n",
    "# split-oriented DataFrame input\n",
    "$ curl http://127.0.0.1:4242/invocations -H 'Content-Type: application/json' -d '{\n",
    "  \"dataframe_split\": {\n",
    "      \"columns\": [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"],\n",
    "      \"data\": [[1, 3], [14, 120]]\n",
    "  }\n",
    "}'\n",
    "{\"predictions\": [\"Adelie\", \"Adelie\"]}\n",
    "$\n",
    "\n",
    "# record-oriented DataFrame input (fine for vector rows, loses ordering for JSON records)\n",
    "curl http://127.0.0.1:4242/invocations -H 'Content-Type: application/json' -d '{\n",
    "  \"dataframe_records\": [\n",
    "    {\"Culmen Length (mm)\": 1,\"Culmen Depth (mm)\": 3},\n",
    "    {\"Culmen Length (mm)\": 14,\"Culmen Depth (mm)\": 120}\n",
    "  ]\n",
    "}'\n",
    "{\"predictions\": [\"Adelie\", \"Adelie\"]}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96b979b3",
   "metadata": {},
   "source": [
    "Or we can **call the API directly from python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107064bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'predictions': ['Adelie', 'Adelie']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "scoring_uri = \"http://127.0.0.1:4242/invocations\"\n",
    "\n",
    "# `sample_input` is a JSON-serialized pandas DataFrame with the `split` orientation\n",
    "sample_input = {  \n",
    "    \"dataframe_split\": {\n",
    "      \"columns\": [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"],\n",
    "      \"data\": [[1, 3], [14, 120]]\n",
    "    }\n",
    "}\n",
    "response = requests.post(\n",
    "              url=scoring_uri, data=json.dumps(sample_input),\n",
    "              headers={\"Content-type\": \"application/json\"})\n",
    "\n",
    "print(response.status_code)\n",
    "#print(response.text)\n",
    "response_json = json.loads(response.text)\n",
    "print(response_json)\n",
    "\n",
    "# 200\n",
    "# {'predictions': ['Adelie', 'Adelie']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1e9787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'predictions': ['Adelie', 'Adelie']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "scoring_uri = \"http://127.0.0.1:4242/invocations\"\n",
    "\n",
    "# `sample_input` is a record-oriented DataFrame input (fine for vector rows, loses ordering for JSON records)\n",
    "sample_input = {  \n",
    "    \"dataframe_records\": [\n",
    "      {\"Culmen Length (mm)\": 1,\"Culmen Depth (mm)\": 3},\n",
    "      {\"Culmen Length (mm)\": 14,\"Culmen Depth (mm)\": 120}\n",
    "    ]\n",
    "}\n",
    "response = requests.post(\n",
    "              url=scoring_uri, data=json.dumps(sample_input),\n",
    "              headers={\"Content-type\": \"application/json\"})\n",
    "\n",
    "print(response.status_code)\n",
    "#print(response.text)\n",
    "response_json = json.loads(response.text)\n",
    "print(response_json)\n",
    "\n",
    "# 200\n",
    "# {'predictions': ['Adelie', 'Adelie']}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f7bd40f",
   "metadata": {},
   "source": [
    "**Serving with MLServer**\n",
    "\n",
    "Python models can be deployed using [Seldon’s MLServer](https://mlserver.readthedocs.io/en/latest/) as alternative inference server. \n",
    "\n",
    "MLServer is integrated with two leading open source model deployment tools: \n",
    "\n",
    "- [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/graph/protocols.html#v2-kfserving-protocol) and \n",
    "- [KServe (formerly known as KFServing)](https://kserve.github.io/website/modelserving/v1beta1/sklearn/v2/), \n",
    "\n",
    "and can be used to test and deploy models using these frameworks. **This is especially powerful when building docker images since the docker image built with MLServer can be deployed directly with both of these frameworks.**\n",
    "\n",
    "MLServer exposes the same scoring API through the `/invocations` endpoint. In addition, it supports the standard V2 Inference Protocol.\n",
    "\n",
    "> **Note:**\n",
    ">\n",
    "> To use MLServer with MLflow, please install mlflow as:\n",
    ">\n",
    "> ```bash\n",
    "> $ pip install mlflow[extras]\n",
    "> ```\n",
    "\n",
    "To serve a MLflow model using MLServer, you can use the --enable-mlserver flag, such as:\n",
    "\n",
    "```bash\n",
    "(mlflow)$ mlflow models serve -m my_model --enable-mlserver\n",
    "```\n",
    "\n",
    "Similarly, to build a Docker image built with MLServer you can use the `--enable-mlserver` flag, such as:\n",
    "\n",
    "```bash\n",
    "(mlflow)$ mlflow models build -m my_model --enable-mlserver -n my-model\n",
    "```\n",
    "\n",
    "To read more about the integration between MLflow and MLServer, please check the [end-to-end example in the MLServer documentation](https://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html) or visit the [MLServer docs](https://mlserver.readthedocs.io/en/latest/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1505f496",
   "metadata": {},
   "source": [
    "## 2.6. Other deployment targets\n",
    "\n",
    "- **AzureML**: [Deploy a python_function model on Microsoft Azure ML](https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-microsoft-azure-ml) \n",
    "- **Sagemaker**: [Deploy a python_function model on Amazon SageMaker](https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker)\n",
    "- Kubernetes\n",
    "- ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7df36042",
   "metadata": {},
   "source": [
    "## 2.7. Transition a models stages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e905ed9",
   "metadata": {},
   "source": [
    "Over the course of the model’s lifecycle, a model evolves—from development to staging to production. \n",
    "\n",
    "You can transition a registered model to one of the stages: **Staging, Production or Archived.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393456a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=1,\n",
    "    stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d18dcf5",
   "metadata": {},
   "source": [
    "<img src='../img/mlflow_ui_pinguins_model_version_1_promoted_to_production.png' alt='' width='1000'>\n",
    "\n",
    "<img src='../img/mlflow_ui_pinguins_model_version_1_promoted_to_production_2.png' alt='' width='1000'>\n",
    "\n",
    "<img src='../img/mlflow_ui_pinguins_model_version_1_promoted_to_production_3.png' alt='' width='1000'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "06edfb0a5c746fc39005b6ae5d5bc93de8214c7bd0d55b767355d186d7ff4fbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
